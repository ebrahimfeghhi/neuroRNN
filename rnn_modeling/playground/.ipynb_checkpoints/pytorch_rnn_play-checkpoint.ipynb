{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.anomaly_mode.set_detect_anomaly at 0x1082f7fd0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torchvision \n",
    "import torch.nn as nn\n",
    "import torch.optim as optim \n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from tqdm import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "import math\n",
    "torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFD(Dataset):\n",
    "    \n",
    "    def __init__(self, num_trials, trial_length, coh, start_delay, check_length, mid_delay, target_length, post_delay):\n",
    "        \n",
    "        self.num_trials = num_trials\n",
    "        self.trial_length = trial_length\n",
    "        self.coh = coh\n",
    "        self.start_delay = start_delay\n",
    "        self.check_length = check_length\n",
    "        self.mid_delay = mid_delay\n",
    "        self.target_length = target_length\n",
    "        self.post_delay = post_delay\n",
    "        \n",
    "        X = np.zeros((self.num_trials, self.trial_length))\n",
    "        \n",
    "        # create useful variables\n",
    "        num_coh = self.num_trials // len(self.coh)\n",
    "        target_onset = self.start_delay + self.check_length + self.mid_delay\n",
    "        \n",
    "        # uniformly assign coherences \n",
    "        X[:, self.start_delay:self.start_delay + self.check_length] = np.tile(np.repeat(self.coh, num_coh), (self.check_length,1)).T\n",
    "        \n",
    "        # set target location, -1 means red target is on left, 1 means red target is on right\n",
    "        X[:, target_onset:target_onset+self.target_length] = np.tile([-1,1], (self.target_length, num_trials//2)).T\n",
    "        \n",
    "        Y = np.zeros((self.num_trials, self.trial_length, 2))\n",
    "        \n",
    "        # green checkerboard trials, 0 is left reach, 1 is right reach\n",
    "        Y[:self.num_trials//2, target_onset:target_onset+self.target_length, 0] = np.tile([0,1], (self.target_length, num_trials//4)).T\n",
    "        Y[:self.num_trials//2, target_onset:target_onset+self.target_length, 1] = np.tile([1,0], (self.target_length, num_trials//4)).T\n",
    "        \n",
    "        # red checkerboard trials\n",
    "        Y[self.num_trials//2:, target_onset:target_onset+self.target_length, 0] = np.tile([1,0], (self.target_length, num_trials//4)).T\n",
    "        Y[self.num_trials//2:, target_onset:target_onset+self.target_length, 1] = np.tile([0,1], (self.target_length, num_trials//4)).T\n",
    "        \n",
    "        self.data = torch.from_numpy(np.expand_dims(X,axis=-1)).float()\n",
    "        self.labels = torch.from_numpy(Y).float()\n",
    "        \n",
    "    def __len__(self):\n",
    "        \n",
    "        return (self.data.shape[0])\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        return self.data[idx], self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_data(CFD_object, show_all):\n",
    "    \n",
    "    data = CFD_object.data\n",
    "    labels = CFD_object.labels\n",
    "    \n",
    "    num_trials = len(data)\n",
    "    coh = CFD_object.coh\n",
    "    \n",
    "    if show_all:\n",
    "        for i in range(num_trials):\n",
    "            fig, axs = plt.subplots(1,3,sharey=True)\n",
    "            axs[0].plot(data[i, :])\n",
    "            axs[1].plot(labels[i, :, 0], color='r')\n",
    "            axs[2].plot(labels[i, :, 1], color='g')\n",
    "            plt.show()\n",
    "\n",
    "    else: \n",
    "    \n",
    "        print(\"Red target on left\")\n",
    "        for i in range(0, num_trials, num_trials//len(coh)):\n",
    "            fig, axs = plt.subplots(1,3,sharey=True)\n",
    "            axs[0].plot(data[i, :])\n",
    "            axs[1].plot(labels[i, :, 0], color='r')\n",
    "            axs[2].plot(labels[i, :, 1], color='g')\n",
    "            plt.show()\n",
    "\n",
    "        print(\"Green target on left\")\n",
    "        for i in range(1, num_trials, num_trials//len(coh)):\n",
    "            fig, axs = plt.subplots(1,3,sharey=True)\n",
    "            axs[0].plot(data[i, :])\n",
    "            axs[1].plot(labels[i, :, 0], color='r')\n",
    "            axs[2].plot(labels[i, :, 1], color='g')\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "c = CFD(4, 100, [.1,.3,.7,.9], 0, 50, 0, 50, 0)\n",
    "#visualize_data(c, show_all=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------- RNN ---------\n",
    "# model-related\n",
    "input_size = 1 # features\n",
    "sequence_length = 20 # timesteps\n",
    "hidden_size = 5 # RNN units\n",
    "num_layers = 1 # RNN layers\n",
    "output_dim = 2\n",
    "nonlin='relu'\n",
    "\n",
    "# training-related\n",
    "num_epochs = 2\n",
    "learning_rate = 1e-3\n",
    "batch_size = 10\n",
    "lambda_omega = 2\n",
    "lambda_l2 = 1\n",
    "clip_value = .1\n",
    "\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_dim, ratio):\n",
    "        super(RNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.input_size = input_size\n",
    "        self.num_layers = num_layers\n",
    "        self.output_dim = output_dim\n",
    "        self.ratio = ratio \n",
    "        self.rnn = nn.RNN(input_size, hidden_size, num_layers, nonlinearity=nonlin, batch_first=True, bias=False)\n",
    "        self.fc = nn.Linear(hidden_size, output_dim, bias=False)\n",
    "        \n",
    "        \n",
    "    def dale_weight_init(self):\n",
    "        \n",
    "        with torch.no_grad():\n",
    "        \n",
    "            num_exc = np.int(self.ratio[0]*self.hidden_size)\n",
    "            num_inh = np.int(self.hidden_size - num_exc)\n",
    "\n",
    "            D = torch.diag_embed(torch.cat((torch.ones(num_exc), -1*torch.ones(num_inh)))) \n",
    "            self.rnn.weight_hh_l0 = torch.nn.Parameter(torch.abs(self.rnn.weight_hh_l0.detach()).matmul(D))\n",
    "                                                    \n",
    "\n",
    "    def enforce_dale(self):\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            \n",
    "            num_exc = np.int(self.ratio[0]*self.hidden_size)\n",
    "            num_inh = np.int(self.hidden_size - num_exc)\n",
    "\n",
    "            self.rnn.weight_hh_l0[:num_exc, :].clamp(min=0)\n",
    "            self.rnn.weight_hh_l0[num_exc:, :].clamp(max=0)\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size, requires_grad=True).to(device)\n",
    "        h_t, _ = self.rnn(x,h0)\n",
    "        h_t.retain_grad()\n",
    "        self.h_t = h_t\n",
    "        print(h_t.shape)\n",
    "        out = self.fc(h_t)\n",
    "        return out\n",
    "    \n",
    "\n",
    "model = RNN(input_size, hidden_size, num_layers, output_dim, ratio=[.8,.8,.8]).float()\n",
    "#model.dale_weight_init()\n",
    "       \n",
    "# loss function and gradient desc. algorithm \n",
    "criterion = nn.L1Loss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate, weight_decay=lambda_l2)\n",
    "\n",
    "training_set = CFD(1000, sequence_length, [.1,.9], 0, 10, 0, 10, 0)\n",
    "trainloader = DataLoader(training_set, batch_size=batch_size,\n",
    "                         shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pascanu_regularizer(model, nonlin):\n",
    "    \n",
    "    '''\n",
    "    \n",
    "     Step 1: compute gradient of ht w.r.t ht-1\n",
    "         This equals Wrec.T*I(Winxt + Wrecht-1 + b), I(x) = 0 if x <= 0, and 1 else\n",
    "         Compute I(Winxt + Wrecht-1 + b) from ht\n",
    "\n",
    "     Step 2: multiply BPTT computed dloss/dht w/ dht/dht-1 \n",
    "     \n",
    "     Step 3: calculate omega\n",
    "     \n",
    "    '''\n",
    "    global first_run\n",
    "    \n",
    "    if first_run:\n",
    "        first_run=False\n",
    "        return 0\n",
    "    \n",
    "    if nonlin == 'relu':\n",
    "            \n",
    "        # Step 1 \n",
    "        h_t_binary = torch.squeeze(torch.diag_embed(torch.squeeze(model.h_t.grad))) # diag matrix, hidden_size*hidden_size \n",
    "        h_t_binary[h_t_binary!=0] = 1 # convert to I(...) \n",
    "        dht_dht_prev = torch.matmul(model.rnn.weight_hh_l0.T, h_t_binary) # seq_len x hidden_size * hidden_size\n",
    "\n",
    "        # Step 2 \n",
    "        dl_dht_prev = torch.squeeze(torch.bmm(torch.unsqueeze(torch.squeeze(model.h_t.grad),axis=1), dht_dht_prev)) # seq_len x hidden_size\n",
    "\n",
    "        # Step 3\n",
    "        omega = torch.sum(torch.pow((torch.norm(dl_dht_prev,dim=1) / (torch.norm(torch.squeeze(model.h_t.grad),dim=1))) - 1, 2))\n",
    "        return omega \n",
    "\n",
    "    else:\n",
    "        \n",
    "        print(\"Code has not been written for other nonlin functions yet, omega is 0\")\n",
    "        omega = 0 \n",
    "        \n",
    "    return omega"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 1])\n",
      "torch.Size([10, 1])\n"
     ]
    }
   ],
   "source": [
    "first_run = True\n",
    "i = 0\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    for idx, (data, targets) in enumerate(trainloader):\n",
    "        \n",
    "        data = data.to(device=device)\n",
    "\n",
    "        targets = targets.to(device=device)\n",
    "        \n",
    "        omega = pascanu_regularizer(model, nonlin)\n",
    "        \n",
    "        # forward\n",
    "        scores = model(data)\n",
    "        l1_loss = criterion(scores, targets)\n",
    "        \n",
    "        # backward\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        l1_loss.backward()\n",
    "        print(l1_loss)\n",
    "        print(model.rnn.weight_hh_l0.grad)\n",
    "        \n",
    "        if omega != 0:\n",
    "            print(\"----------omega-----------\")\n",
    "            omega.backward()\n",
    "            print(model.rnn.weight_hh_l0.grad)\n",
    "            print(\"\\n\")\n",
    "        \n",
    "\n",
    "\n",
    "        \n",
    "        # clip gradients \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip_value)\n",
    "    \n",
    "        # use saved gradients in params to step \n",
    "        optimizer.step()\n",
    "        \n",
    "        # impose dale's principle\n",
    "        #model.enforce_dale()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "omega.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rnn.weight_ih_l0\n",
      "rnn.weight_hh_l0\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if 'rnn'in name:\n",
    "        print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0100)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# examining loss function \n",
    "criterion = nn.L1Loss()\n",
    "pred = torch.zeros(100, 2)\n",
    "pred[50:, 0] = .96\n",
    "label = torch.zeros(100, 2)\n",
    "label[50:, 0] = 1\n",
    "criterion(pred,label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.,  0.,  1.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "device = 'cpu'\n",
    "a = torch.tensor([-10,0,20], dtype=torch.float64, device=device, requires_grad=True)\n",
    "b = torch.tensor([0,0,0], dtype=torch.float64, device=device)\n",
    "loss = torch.nn.functional.l1_loss(a, b, reduction='sum')\n",
    "loss.backward()\n",
    "print(a.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[2., 1.],\n",
       "         [2., 1.]],\n",
       "\n",
       "        [[2., 1.],\n",
       "         [2., 1.]],\n",
       "\n",
       "        [[2., 1.],\n",
       "         [2., 1.]],\n",
       "\n",
       "        [[2., 1.],\n",
       "         [2., 1.]],\n",
       "\n",
       "        [[2., 1.],\n",
       "         [2., 1.]]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Wrec = torch.ones(2,2)\n",
    "h_t_binary = torch.ones(5,2,2)\n",
    "h_t_binary[:,1, 1] = 0\n",
    "torch.matmul(Wrec.T, h_t_binary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.,  1.,  1., -1., -1.],\n",
      "        [ 1.,  1.,  1., -1., -1.]])\n"
     ]
    }
   ],
   "source": [
    "b = torch.ones(1,5)\n",
    "e = torch.ones(2,5)\n",
    "f = torch.matmul(e,d)\n",
    "print(f)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN (nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, output_dim):\n",
    "        super(NN, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.output_dim = output_dim\n",
    "        self.fc = nn.Linear(input_size, output_dim, bias=False)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.fc(x)\n",
    "        return out\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<SqueezeBackward3 at 0x120777f10>"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vanilla_NN = NN(10,1)\n",
    "out = vanilla_NN(2*torch.ones(10))\n",
    "out.backward()\n",
    "out.grad_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5.2915)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l2 = torch.Tensor([1,2,3,1,2,3])\n",
    "torch.norm(l2)\n",
    "torch.sum(l2**2)**.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "size mismatch, m1: [10 x 10], m2: [32 x 10] at /Users/distiller/project/conda/conda-bld/pytorch_1595629430416/work/aten/src/TH/generic/THTensorMath.cpp:41",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-d379bcdcba66>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: size mismatch, m1: [10 x 10], m2: [32 x 10] at /Users/distiller/project/conda/conda-bld/pytorch_1595629430416/work/aten/src/TH/generic/THTensorMath.cpp:41"
     ]
    }
   ],
   "source": [
    "torch.mm(torch.ones(10,10), torch.ones(32,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
